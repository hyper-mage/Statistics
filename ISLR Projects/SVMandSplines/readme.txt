Support Vector Machines can be used for both regression and classification. SVMs can be extended into
non-linear space by applying different kernels to the function. Polynomial and radial basis functions
are explored in this project. The idea behind SVM is simply finding a dividing line between classes.
We add a level of error by allowing some degree of misclassification in the margins of our hyperplanes.
This creates a "slab" which is wedged between observations of opposing classes, these observations
become the "support" for our SVM.

Cubic regression is also explored in this project. Compared with polynomial regression and supplemented
with smoothing and natural splines.
