---
title: "STAT 6340 Mini Project 5"
author: "Matthew Lynn"
date: "April 10, 2019"
output:
  html_document:
    theme: sandstone
    df_print: paged
  pdf_document: default
  word_document: default
---
<style type="text/css">

h1.title {
  font-size: 40px;
  color: DarkRed;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
    font-size: 18px;
  <!-- font-family: "Times New Roman", Times, serif; -->
  color: DarkRed;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 18px;
  <!-- font-family: "Times New Roman", Times, serif; -->
  color: DarkBlue;
  text-align: center;
}
</style>
# Section 1
#### Section 2 is coded in Section 1
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align='center',
                      warning=FALSE, message=FALSE)
```

## 1(a)
We look at the Caravan dataset to determine how different the units  
each variable is. No suprise, we have vastly different scales for our  
dataset. We want to standardize the data such that each variable is  
fairly compared with all other variables. We use Purchase as our  
response variable due to being qualitative.
```{r}
library(ISLR)
caravan = Caravan
s.caravan = scale(caravan[, -86]) #86 column is purchase
c(var(caravan[,1]),var(caravan[,2]),var(s.caravan[,1]),var(s.caravan[,2]))
x = model.matrix(Purchase~., caravan)[,-1]
y = caravan$Purchase
```

### 1(b)
Here, we simply split the data into test and training. Our first 1000 obs  
will be our test set while the rest are training.
```{r}
test = 1:1000
train = (-test)
```

### 1(c)
We fit a logistic regression model with no special features except instead  
of a 0.5 cutoff for the probability a person will make the purchase,  
we use a 0.2. Additionally, we compute the confusion matrix,  
sensitivity, specificity, and  
overall misclassification rate based on the test data.
```{r}
library(caret)
set.seed(1)
tc <- trainControl()
model <- train(x[train,], y[train],
               method="glm", family="binomial", trControl = tc)
model
model = glm(Purchase~., data = caravan, family = binomial, subset = train)
prob = predict(model, caravan[test,], type = "response")
pred <- ifelse(prob >= 0.2, "Yes", "No")
confusionMatrix(table(pred, y[test]))
LR.ERR = mean(pred != y[test]) # Overall misclassification rate
```

### 1(d)
We repeat the procedure from 1(c) but use a penalty function instead,  
ridge-regression. Our best lambda is rather small, perhaps a normal  
logistic regression function is not so bad in comparison.  
*Note: cv.glmnet defaults to 10-fold cross validation.*  
The graph below shows level of variation per lambda as well as how many  
coefficients denoted along the top axis. The dotted lines show our best lambdas.
```{r}
set.seed(1)
library(glmnet)
lambdas <- 10^seq(10, -2, length = 100)
ridge.model <- glmnet(x[train,], y[train], alpha = 0, lambda = lambdas,
                      family = "binomial")
ridge.cv <- cv.glmnet(x[train,], y[train], alpha = 0, family = "binomial")
plot(ridge.cv)
# Best cross-validated lambda
lambda.cv = ridge.cv$lambda.min
```
Lambda is `r lambda.cv`, which is quite small. We may as well use least squares.
```{r}
prob = predict(ridge.model, s = lambda.cv, newx = x[test,], type = "response")
pred <- ifelse(prob >= 0.2, "Yes", "No")
confusionMatrix(table(pred, y[test]))
# the predict function below specifies the best model by using optimal lambda
coef = predict(ridge.model, x[test,], type = "coefficients", s = lambda.cv)
length(coef[coef != 0]) # ridge uses all variables
RIDGE.ERR = mean(pred != y[test]) # Overall misclassification rate
```

### 1(e)
We repeat the procedure from 1(c) but use a penalty function instead,  
lasso-regression. Our graph looks rather different, and has decreasing coefficients.
```{r}
set.seed(1)
lasso.model <- glmnet(x[train,], y[train], alpha = 1, lambda = lambdas,
                      family = "binomial")
lasso.cv <- cv.glmnet(x[train,], y[train], alpha = 1, family = "binomial")
plot(lasso.cv)
# Best cross-validated lambda
lambda.cv = lasso.cv$lambda.min
```
Again, lambda is small, `r lambda.cv`, we may as well use least squares.  
But, we will see that Lasso greatly reduces our complexity.
```{r}
prob = predict(lasso.model, s = lambda.cv, newx = x[test,], type = "response")
pred <- ifelse(prob >= 0.2, "Yes", "No")
confusionMatrix(table(pred, y[test]))
coef = predict(lasso.model, x[test,], type = "coefficients", s = lambda.cv)
length(coef[coef != 0]) # lasso only uses 12 variables
LASSO.ERR = mean(pred != y[test]) # Overall misclassification rate
```

### 1(f)
Lasso and normal Logistic Regression share the same overall misclassification.  
Ridge uses all variables and did not do any better than LR.  
Lasso managed to only use 12 variables and achieve the same results except  
less specificity. We would proceed with Lasso due to using a significantly  
smaller model while maintaining high model accuracy (lowest error rate in fact).
```{r}
round(cbind(LR.ERR,RIDGE.ERR, LASSO.ERR), 3) # Test error rates

```

## 2(a)
We perform simple exploratory data analysis on the dataset mali.  
Family vs DistRD is practically 0  
DistRD vs Cattle is practically 0  
DistRD is practically 0 with any other predictor, can probably drop  
All other variables may be of use for predicting.
```{r}
mali = read.csv("mali.csv", header = T)
# eda
str(mali)
pairs(subset(mali))
pairs(subset(mali, select = c(Family, DistRD, Cattle)))
round(cor(mali), 3)
round(cor(subset(mali, select = c(Family, DistRD, Cattle))), 3)
corrplot::corrplot(cor(mali), method = "number", type = "upper")
rbind(apply(mali, 2, mean),apply(mali, 2, var)) # => scale issues
# it is apparent how different they are from the mean/variance
# DistRD doesn't seem to play well with others
boxplot(subset(mali, select = c(Family, DistRD, Cattle)))$out # values of outliers
# we delete the most outlying row from each category, respectively
mali.new = mali[-c(25, 72, 34), ] # these obs are far out
```

### 2(b)
Most of the data is in hectares unit, however not all the data is using  
the same unit measures. Therefore, without a doubt we should standardize!

### 2(c)
We want to use Principal Components Analysis (PCA) to reduce our model  
to its most important characteristics. We look to the Proportion of  
Variation Explained (PVE) and its cumulitive sum and see that  
five to six variables should explain most of our variance (85%-95%).  
The  1st graph below shows in what direction our components traverse,  
we see that Maize & cattle got together and Millet & Sorg go together and etc.  
Notice that this biplot corresponds to the first component vs the second.  
The 2nd graph shows the component scores against each other, there is some  
positive correlation with z1&z2 but not so much with z1&z3.  
The 3rd graph shows the PVE and Cumulative PVE, seems like we only need a few!
```{r}
library(pls)
library(ggbiplot)
require(ggplot2)
set.seed(1)
pr.out = prcomp(mali.new, scale. = T)
rbind(pr.out$center, pr.out$scale)
summary(pr.out)
# biplot(pr.out, scale=0)
ggbiplot(pr.out, obs.scale = 1, var.scale = 1, labels=row.names(mali.new), 
         ellipse = TRUE, circle = TRUE) + 
  scale_color_discrete(name = '') + 
  theme(legend.direction = 'horizontal', legend.position = 'top')
pr.var = pr.out$sdev^2
pve = pr.var/sum(pr.var)
pve
par(mfrow = c(1,2))
plot(pr.out$x[,1:2], xlab = "z1", ylab = "z2")
plot(pr.out$x[,c(1,3)], xlab = "z1", ylab = "z3")
plot(pve, xlab = "Principal Component", ylab = "PVE", ylim = c(0,1), type = 'b')
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative PVE", ylim = c(0,1), type = 'b')
```

### 2(d)
Based on the biplot above and the rotation matrix below, we see that  
Family, Cotton, and Bull each have similar weight.  
It appears each component corresponds to a measure of farmer's resource  
density.
```{r}
pr.out$rotation[,1:2]
summary(pr.out)$importance[2:3,1:2]
```

### 2(e)
Component 1: Farmers with multi-puropose farms  
Component 2: Farmland specifically used for cereal/grain crops  
Component 3: Goats coincide with roads, perhaps by being more free-range  
Component 4: Similar to component 3 with increases to cattle and maize  
Component 5: Similar to component 2
```{r}
pr.out$rotation[,1:5]
```

## 3(a)
Here we fit a regular linear model
```{r}
auto = subset(ISLR::Auto, select = -name) # exclude name because it gets nasty
set.seed(1)
x<-model.matrix(mpg~., auto)[,-1]
y<-auto$mpg
fit = lm(y~x)
summary(fit)
pred = predict(fit, newx = x)
LS.ERR = mean((pred - y)^2) # test MSE (same as train MSE)
LS = round(summary(fit)$coefficients[,1], 3)
```

### 3(b)
Now we perform best subset and see what we get
```{r}
set.seed(1)
totpred <- ncol(auto) - 1
fit <- leaps::regsubsets(mpg~., auto, nvmax = totpred)
fit.summary <- summary(fit)
fit.summary
point = as.numeric(which.min(fit.summary$bic)) # graph gives best model (not shown)
# function required to perform prediction on regsubsets
predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    mat[, names(coefi)] %*% coefi
}
pred = predict.regsubsets(fit, auto, point)
BS.ERR = mean((pred - y)^2)
BS = round(coef(fit, point), 3) # only uses 3 (from point)
Term = names(coef(fit, 7)) # to be used later
```

### 3(c)
Ridge Resgression, the graph is used as in question 1 for determining best lambda.
```{r}
set.seed(1)
require(glmnet)
lambdas <- 10^seq(10, -2, length = 100)
x<-model.matrix(mpg~., auto)[,-1]
y<-auto$mpg
fit <- glmnet(x, y, alpha = 0, lambda = lambdas, family = "gaussian")
fit.cv <- cv.glmnet(x, y, alpha = 0)
plot(fit.cv)
# Best cross-validated lambda
lambda = fit.cv$lambda.min
pred = predict(fit, s = lambda, newx = x)
RIDGE.ERR = mean((pred - y)^2)
coef = predict(fit, x, type = "coefficients", s = lambda)
Ridge = round(coef[,1], 3)
length(coef[coef != 0]) # ridge uses all variables
```

### 3(d)
Lasso Regression, graph below appears to show 4 coefficients but the model chosen  
included all predictors. This may be an error in coding or the fact that the function  
decided the seemingly non existent decreaase in MSE was sufficient.
```{r}
set.seed(1)
lambdas <- 10^seq(10, -2, length = 100)
fit <- glmnet(x, y, alpha = 1, lambda = lambdas, family = "gaussian")
fit.cv <- cv.glmnet(x, y, alpha = 1)
plot(fit.cv)
# Best cross-validated lambda
lambda = fit.cv$lambda.min
pred = predict(fit, s = lambda, newx = x)
LASSO.ERR = mean((pred - y)^2)
coef = predict(fit, x, type = "coefficients", s = lambda)
Lasso = round(coef[,1], 3)
length(coef[coef != 0]) # lasso uses all variables
```

### 3(e)
Now we use Principal Component Regression, the graph below is similar to those used  
in auto selection methods. From a viewer point, 3 predictors seem to be ideal.  
However, the function decided on a full model fit similar to 3(d)'s model.
```{r}
require(pls)
set.seed(1)
fit = pcr(mpg~., data = auto, scale = T, validation = "CV")
summary(fit)
validationplot(fit, val.type = "MSEP")
fit = pcr(mpg~., data = auto, scale = T, ncomp = 7)
pred = predict(fit, newx = x)
PCR.ERR = mean((pred - y)^2)
summary(fit)
PCR = round(coef(fit, intercept = T), 3)
```

### 3(f)
Finally we do Partial Least Squares.  
Here we use less components because PLS explains the predictor more effectively.  
Graph is smoother than 3(e)'s but we conclude similar reasonings except PLS does  
suggest just two components in comparison to PCR which chose all components.
```{r}
set.seed(1)
fit = plsr(mpg~., data = auto, scale = T, validation = "CV")
summary(fit)
validationplot(fit, val.type = "MSEP") # 2 components look good
fit = plsr(y~x, data = auto, scale = T, ncomp = 2)
pred = predict(fit, newx = x)
PLS.ERR = mean((pred - y)^2)
summary(fit)
PLS = round(coef(fit, intercept = T), 3)
```

### 3(g)
Below is a table representing the weights of each coefficient for each of the  
different models alongside their test MSE. We see that Best Subset Selection  
results in fewer coefficients being used but Partial Least Squares resulted  
in the smallest test MSE. We recommend the best subset model because of the  
aforementioned, its test MSE is only 0.7 higher than then PLS's.
```{r}
# round(cbind(LS, BS, Ridge, Lasso, PCR, PLS), 3)
table = rowr::cbind.fill(Term, LS, BS, Ridge, Lasso, PCR, PLS, fill = NA)
colnames(table)<-c("Term", "LS", "BS", "Ridge", "Lasso", "PCR", "PLS")
# note the NA in Term is error row
table = rbind(table, c(LS.ERR, BS.ERR, RIDGE.ERR, LASSO.ERR, PCR.ERR, PLS.ERR))
table
```