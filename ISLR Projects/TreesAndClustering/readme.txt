This project delves into Tree based methods and clustering.

Hierarchical clustering is a clustering method that can be displayed in a tree like plot. One can
determine the amount of clusters to be shown by cutting at a specific height, the higher you cut the
less clusters you will have.

K-means Clustering is, as the name says, a clustering method. This has similarities to KNN such that
we are clustering observations based on a distance metric, but instead of regression we are
classifying.

Decision Trees split the data into regions that can be used as a classifier. Trees can get very
messy, so CV can be used to "prune" the tree. The idea is the same as any with respect to complexity.
We want to shave off as much unnecessary information without destroying the model.

Methods such as Bagging, Boosting, Random Forest give Decision Trees a lot of flexibility.
All of these methods are explored in this project.
